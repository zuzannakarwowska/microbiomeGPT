{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d99809-b908-4013-ab42-095351673917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core Data Science Libraries ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Visualization ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === PyTorch: Deep Learning ===\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# PyTorch Distributions\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "# PyTorch Metrics\n",
    "from torchmetrics.regression import MeanSquaredError, MeanAbsolutePercentageError\n",
    "\n",
    "# === Machine Learning: Scikit-learn ===\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, precision_score, recall_score, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# === Statistical & Distance-Based Methods ===\n",
    "from scipy.spatial.distance import braycurtis, pdist, squareform\n",
    "from scipy.spatial import procrustes\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, f_oneway, ttest_ind\n",
    "\n",
    "# Multiple Hypothesis Testing\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Ordination Methods\n",
    "from skbio.stats.ordination import pcoa\n",
    "\n",
    "# Distance-Based Statistical Tests\n",
    "from skbio.stats.distance import permanova\n",
    "from skbio import DistanceMatrix\n",
    "\n",
    "# === Utility Libraries ===\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# === Hyperparameter Tuning & Distributed Training ===\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f56301b-03e1-4520-bd95-d723ba2b92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296ccade-4d4e-4ff5-869f-ee45618e0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"  # Limit to 2 threads\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0c58ba-b64c-4524-b7ff-e91230f34441",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c8867a-6516-445e-b3bf-449456d0f847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zuzannak/MicroFormer\n"
     ]
    }
   ],
   "source": [
    "%cd /home/zuzannak/MicroFormer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "76e3d096-1a49-4115-9191-ac93fbc0f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = pd.read_csv('inputs/taxonomy_relab.csv', index_col = [0], low_memory=False).fillna(0).sort_index()\n",
    "pathways = pd.read_csv('inputs/pathways_relab.csv', index_col = [0], low_memory=False).fillna(0).sort_index()\n",
    "metadata = pd.read_csv('inputs/metadata.csv', index_col= [0], low_memory=False).sort_index()\n",
    "\n",
    "pathways_sum           = pathways.astype(bool).astype(int).sum(axis=1).sort_values()\n",
    "pathways_keep_subjects = pathways_sum[pathways_sum > pathways_sum.quantile(.1)].index\n",
    "pathways               = pathways.loc[pathways_keep_subjects]\n",
    "\n",
    "taxonomy_sum           = taxonomy.astype(bool).astype(int).sum(axis=1).sort_values()\n",
    "taxonomy_keep_subjects = taxonomy_sum[taxonomy_sum > taxonomy_sum.quantile(.1)].index\n",
    "taxonomy               = taxonomy.loc[taxonomy_keep_subjects]\n",
    "\n",
    "keep_idx   = list(set(pathways.index).intersection(taxonomy.index))\n",
    "metadata   = metadata[metadata['sample_id'].isin(keep_idx)]\n",
    "taxonomy   = taxonomy.loc[keep_idx]\n",
    "pathways   = pathways.loc[keep_idx]\n",
    "\n",
    "common_idx = list(set(taxonomy.index).intersection(pathways.index))\n",
    "\n",
    "metadata   = metadata[metadata['sample_id'].isin(common_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e49b4e8c-6131-4415-a907-83ab27af505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['SICK'] = np.where(metadata.study_condition == 'control', 0, 1)\n",
    "\n",
    "keep_samples_H   = metadata[(metadata['SICK'] == 0)].drop_duplicates('sample_id', keep='first').sample(1000, random_state=32).sample_id.tolist()\n",
    "keep_samples_S   = metadata[(metadata['SICK'] == 1)].drop_duplicates('sample_id', keep='first').sample(1000).sample_id.tolist()\n",
    "\n",
    "keep_samples     = keep_samples_H + keep_samples_S\n",
    "\n",
    "taxonomy         = taxonomy.loc[keep_samples]\n",
    "pathways         = pathways.loc[keep_samples].iloc[:, 2:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fca2c517-de2e-4ff2-b5d2-6e504ca95f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_prevalence(df, treshold = 0.1):\n",
    "    '''features as columns'''\n",
    "    df_binary = df.copy()\n",
    "    df_binary[df_binary>0]=1\n",
    "    df_binary_sum = df_binary.sum(axis=0)\n",
    "    \n",
    "    keep_features = df_binary_sum[df_binary_sum > df.shape[0]*treshold].index\n",
    "    filtered_df = df[keep_features]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "filtered_taxonomy = filter_prevalence(taxonomy)\n",
    "filtered_pathways = filter_prevalence(pathways, .1)\n",
    "\n",
    "filtered_taxonomy = filtered_taxonomy[~filtered_taxonomy.index.duplicated(keep='first')]\n",
    "filtered_pathways = filtered_pathways[~filtered_pathways.index.duplicated(keep='first')]\n",
    "\n",
    "filtered_taxonomy = filtered_taxonomy.div(filtered_taxonomy.sum(axis=1), axis=0)\n",
    "filtered_pathways = filtered_pathways.div(filtered_pathways.sum(axis=1), axis=0)\n",
    "\n",
    "filtered_pathways = filtered_pathways.dropna()\n",
    "filtered_taxonomy = filtered_taxonomy.dropna()\n",
    "\n",
    "common_idx        = list(set(filtered_pathways.index).intersection(filtered_taxonomy.index))\n",
    "\n",
    "filtered_taxonomy = filtered_taxonomy.loc[common_idx].sort_index()\n",
    "filtered_pathways = filtered_pathways.loc[common_idx].sort_index()*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2a03048a-e444-4279-9107-da316ddd552b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 409), (6000, 196))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_pathways.shape, filtered_taxonomy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d7456ee4-767c-4d23-baa6-40ee6702d493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0), np.int64(0))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_pathways.isna().sum().sum(), filtered_taxonomy.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83fd93d9-1463-4398-abc1-d083b68bbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BacteriaModel(nn.Module):\n",
    "    def __init__(self, num_pathways, num_bacteria, embedding_dim, latent_size):\n",
    "        super(BacteriaModel, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        # Bacteria encoding\n",
    "        self.pathways_embedding = nn.Embedding(num_pathways, embedding_dim) \n",
    "        self.taxonomy_embedding = nn.Embedding(num_bacteria, embedding_dim) #czy wektory embeddingow koreluje z filogenetyka czy on mimo braku informacji rozumie filogenetyke\n",
    "        \n",
    "        # Linear layer to transform (B, #Bac, 1) to (B, #Bac, D)\n",
    "        self.linear_layer = nn.Linear(1, embedding_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=2, batch_first=True, dropout=0.05)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "\n",
    "        self.final_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=embedding_dim, batch_first=True)\n",
    "        self.final_transformer_encoder = nn.TransformerEncoder(self.final_encoder_layer, num_layers=1)\n",
    "        \n",
    "        # Mean and Logvar layers\n",
    "        self.latent_layer = nn.Linear(embedding_dim, latent_size)\n",
    "        \n",
    "        # Linear transformation for latent vector\n",
    "        self.linear_transform = nn.Linear(latent_size, embedding_dim)\n",
    "        \n",
    "        # Transformer decoder\n",
    "        self.decoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=2, batch_first=True) \n",
    "        self.transformer_decoder = nn.TransformerEncoder(self.decoder_layer, num_layers=1) \n",
    "        \n",
    "        # Final linear layer to transform (B, #Bac, D) to (B, #Bac, 1)\n",
    "        self.latent_norm = nn.LayerNorm(latent_size)\n",
    "\n",
    "        self.output_transform = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 1))\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                \n",
    "    \n",
    "    def forward(self, pathways_tensor, bacteria_tensor):\n",
    "        \n",
    "        batch_size = pathways_tensor.size(0)\n",
    "        num_pathways = pathways_tensor.size(1)\n",
    "        num_bacteria = bacteria_tensor.size(1)\n",
    "\n",
    "        # Step 1: Transform (B, #Bac, 1) to (B, #Bac, D)\n",
    "        pathways_transformed = self.linear_layer(pathways_tensor)\n",
    "        bacteria_transformed = self.linear_layer(bacteria_tensor)\n",
    "        \n",
    "        # Step 2: Create bacteria encoding\n",
    "        pathways_indices = torch.arange(num_pathways).to(pathways_tensor.device)\n",
    "        pathways_encoded = self.pathways_embedding(pathways_indices)\n",
    "        bacteria_indices = torch.arange(num_bacteria).to(bacteria_tensor.device)\n",
    "        bacteria_encoded = self.taxonomy_embedding(bacteria_indices)\n",
    "        \n",
    "        # Expand bacteria encoding to match batch size and add to bacteria_transformed\n",
    "        pathways_encoded_expanded = pathways_encoded.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        pathways_result = pathways_transformed + pathways_encoded_expanded\n",
    "        bacteria_encoded_expanded = bacteria_encoded.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        bacteria_result = bacteria_transformed + bacteria_encoded_expanded\n",
    "        \n",
    "        res_concat = torch.cat([bacteria_result, pathways_result], dim=1)\n",
    "\n",
    "        # Step 3: Pass through Transformer encoder\n",
    "        output = self.transformer_encoder(res_concat)\n",
    "        output = self.final_transformer_encoder(output)\n",
    "\n",
    "        # Step 4: Average over sequence (mean along the sequence dimension)\n",
    "        mean_vector = output.mean(dim=1)\n",
    "\n",
    "        # Step 5: Compute mean and logvar\n",
    "        latent = self.latent_layer(mean_vector)\n",
    "\n",
    "        # Step 7: Transform z to (B, D)\n",
    "        D_vector = self.linear_transform(latent)\n",
    "        \n",
    "        # Step 8: Repeat D_vector for each bacterium to match (B, #Bac, D)\n",
    "        D_vector_expanded = D_vector.unsqueeze(1).repeat(1, num_pathways, 1)\n",
    "        pathways_result_with_encoding = D_vector_expanded + pathways_encoded_expanded\n",
    "\n",
    "        D_vector_expanded_taxonomy = D_vector.unsqueeze(1).repeat(1, num_bacteria, 1)\n",
    "        bacteria_result_with_encoding = D_vector_expanded_taxonomy + bacteria_encoded_expanded\n",
    "        \n",
    "        # Step 9: Pass through Transformer decoder\n",
    "        pathways_output = self.transformer_decoder(pathways_result_with_encoding) #zamienic na encoder i usunąć\n",
    "        bacteria_output = self.transformer_decoder(bacteria_result_with_encoding) #zamienic na encoder i usunąć\n",
    "        \n",
    "        # Step 10: Transform output to (B, #Bac, 1)\n",
    "        pathways_output_transformed = self.output_transform(pathways_output)\n",
    "        bacteria_output_transformed = self.output_transform(bacteria_output)\n",
    "\n",
    "        return bacteria_output_transformed, pathways_output_transformed, latent\n",
    "\n",
    "#Loss function\n",
    "\n",
    "def loss_function(taxonomy_x, taxonomy_x_hat, pathways_x, pathways_x_hat, distance_weight=.1):\n",
    "    \"\"\"\n",
    "    Calculate loss for both taxonomy and pathways outputs.\n",
    "    \n",
    "    Args:\n",
    "        taxonomy_x: Original taxonomy data\n",
    "        taxonomy_x_hat: Reconstructed taxonomy data\n",
    "        pathways_x: Original pathways data\n",
    "        pathways_x_hat: Reconstructed pathways data\n",
    "        mean: Mean of the latent distribution\n",
    "        logvar: Log variance of the latent distribution\n",
    "        beta: Weight for KL divergence loss\n",
    "        distance_weight: Weight for distance preservation loss\n",
    "    \"\"\"\n",
    "    # Basic reconstruction losses for both outputs\n",
    "    taxonomy_reconstruction_loss = F.mse_loss(taxonomy_x_hat, taxonomy_x, reduction='mean')\n",
    "    pathways_reconstruction_loss = F.mse_loss(pathways_x_hat, pathways_x, reduction='mean')\n",
    "    \n",
    "    # Combined reconstruction loss\n",
    "    reconstruction_loss = taxonomy_reconstruction_loss + pathways_reconstruction_loss\n",
    "        \n",
    "    # Distance preservation term for both outputs\n",
    "    def distance_matrix(x):\n",
    "        return torch.cdist(x.squeeze(-1), x.squeeze(-1))\n",
    "    \n",
    "    # Taxonomy distances\n",
    "    taxonomy_true_distances = distance_matrix(taxonomy_x)\n",
    "    taxonomy_rec_distances = distance_matrix(taxonomy_x_hat)\n",
    "    taxonomy_distance_loss = F.mse_loss(taxonomy_rec_distances, taxonomy_true_distances)\n",
    "    \n",
    "    # Pathways distances\n",
    "    pathways_true_distances = distance_matrix(pathways_x)\n",
    "    pathways_rec_distances = distance_matrix(pathways_x_hat)\n",
    "    pathways_distance_loss = F.mse_loss(pathways_rec_distances, pathways_true_distances)\n",
    "    \n",
    "    # Combined distance loss\n",
    "    distance_loss = taxonomy_distance_loss + pathways_distance_loss\n",
    "    \n",
    "    # Variance preservation for both outputs\n",
    "    taxonomy_variance_loss = torch.abs(taxonomy_x.var() - taxonomy_x_hat.var())\n",
    "    pathways_variance_loss = torch.abs(pathways_x.var() - pathways_x_hat.var())\n",
    "    variance_loss = taxonomy_variance_loss + pathways_variance_loss\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = (reconstruction_loss + \n",
    "                  distance_weight * distance_loss + \n",
    "                  0.1 * variance_loss)\n",
    "    \n",
    "    # Return individual loss components for monitoring\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f971a-f490-4853-877b-826fba5191a7",
   "metadata": {},
   "source": [
    "### Param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a19c03ad-5ddb-4bdd-b114-53419b8860fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disparity(ypred_tensor, ytrue_tensor):\n",
    "    \n",
    "    ypred      = ypred_tensor.cpu().detach().numpy()[:, :, 0]\n",
    "    ytrue      = ytrue_tensor.cpu().detach().numpy()[:, :, 0]\n",
    "    \n",
    "    pcoa_ypred = pcoa(squareform(pdist((ypred)))).samples\n",
    "    pcoa_ytrue = pcoa(squareform(pdist((ytrue)))).samples\n",
    "    \n",
    "    mtx1, mtx2, disparity = procrustes(pcoa_ypred, pcoa_ytrue)\n",
    "    \n",
    "    return disparity\n",
    "    \n",
    "\n",
    "def run_anova(z, y):\n",
    "    \n",
    "    '''\n",
    "    Run ANOVA on each latent dimensions between different study conditions\n",
    "\n",
    "    Params\n",
    "    ----------------------\n",
    "    \n",
    "    z: latent layer\n",
    "    y: y variable here study conidition\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    val_z_df = pd.DataFrame(z.cpu().detach().numpy())\n",
    "    val_z_df['y'] = y\n",
    "    \n",
    "    anova_results = [\n",
    "        {\n",
    "            \"latent\": i,\n",
    "            \"stat\": stat,\n",
    "            \"pvalue\": pval\n",
    "        }\n",
    "        \n",
    "        for i in range(z.shape[1])\n",
    "        for stat, pval in [f_oneway(*[group.iloc[:, i].tolist() \n",
    "                                      for _, group in val_z_df.groupby('y')])]\n",
    "    ]\n",
    "    \n",
    "    anova_results_df = pd.DataFrame(anova_results)\n",
    "    anova_results_df['qvalue'] = multipletests(anova_results_df[\"pvalue\"], method=\"fdr_bh\")[1]\n",
    "    \n",
    "    return anova_results_df\n",
    "\n",
    "def run_ttest(z, y):\n",
    "\n",
    "    ''' \n",
    "    Run t-test on latent dimensions between sick and healthy subjects\n",
    "\n",
    "    Params\n",
    "    ----------------------\n",
    "    \n",
    "    z: latent layer\n",
    "    y: y variable here health status 1: sick, 0:sick\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    val_z_df = pd.DataFrame(z.cpu().detach().numpy())\n",
    "    val_z_df['y'] = y\n",
    "    \n",
    "    # Perform t-test for each latent dimension\n",
    "    ttest_results = []\n",
    "    for i in range(z.shape[1]):\n",
    "        x_sick = val_z_df[val_z_df.y == 1][[i]].values\n",
    "        x_healthy = val_z_df[val_z_df.y == 0][[i]].values\n",
    "        \n",
    "        stat, pval = ttest_ind(x_sick, x_healthy, equal_var=False)  # Welch's t-test (unequal variance)\n",
    "        ttest_results.append({\"latent\":i,\n",
    "                              \"stat\":stat[0],\n",
    "                              \"pvalue\":pval[0]})\n",
    "    \n",
    "    \n",
    "    ttest_results_df = pd.DataFrame(ttest_results)\n",
    "    ttest_results_df['qvalue'] = multipletests(ttest_results_df[\"pvalue\"], method=\"fdr_bh\")[1]\n",
    "    \n",
    "    return ttest_results_df\n",
    "\n",
    "def run_permanova(z, y):\n",
    "\n",
    "    \n",
    "    ''' \n",
    "    Run permanova on samples with y as grouping variable\n",
    "\n",
    "    Params\n",
    "    ----------------------\n",
    "    \n",
    "    z: latent layer\n",
    "    y: y variable here health status 1: sick, 0:sick\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    val_z_df = pd.DataFrame(z.cpu().detach().numpy())\n",
    "    dm = squareform(pdist(val_z_df, 'cosine'))\n",
    "    permanova_pvalue = float(permanova(DistanceMatrix(dm), y)['p-value'])\n",
    "\n",
    "    return permanova_pvalue\n",
    "\n",
    "def run_latent_analysis(model, taxonomy_val, pathways_val):\n",
    "\n",
    "    '''\n",
    "    Run tests on latent layer\n",
    "\n",
    "    Params\n",
    "    ----------------------\n",
    "    model: pretrained model\n",
    "    \n",
    "    '''\n",
    "\n",
    "    model = model.cpu()\n",
    " \n",
    "    X = pd.concat([taxonomy_val, pathways_val], axis=1)\n",
    "    \n",
    "    health_status_dict = dict(zip(metadata.sample_id, metadata.SICK))\n",
    "    disease_dict       = dict(zip(metadata.sample_id, metadata.study_condition))\n",
    "    \n",
    "    y_health_status    = X.index.map(health_status_dict).values.reshape(len(X))\n",
    "    y_study_condition  = X.index.map(disease_dict).values.reshape(len(X))\n",
    "    \n",
    "    # Split features\n",
    "    X_taxonomy = X.iloc[:, :taxonomy_val.shape[1]]\n",
    "    X_pathways = X.iloc[:, taxonomy_val.shape[1]:]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    taxonomy_tensor = torch.tensor(X_taxonomy.values).float().unsqueeze(-1).cpu()#.to(DEVICE)\n",
    "    pathways_tensor = torch.tensor(X_pathways.values).float().unsqueeze(-1).cpu()#.to(DEVICE)\n",
    "    \n",
    "    xT_pred, xP_pred, val_z = model(\n",
    "        pathways_tensor, \n",
    "        taxonomy_tensor)\n",
    "    \n",
    "       \n",
    "    anova_df = run_anova(val_z, y_study_condition)\n",
    "    ttest_df = run_ttest(val_z, y_health_status)\n",
    "    \n",
    "    permanova_disease_value = run_permanova(val_z, y_study_condition)\n",
    "    permanova_status_value = run_permanova(val_z, y_health_status)\n",
    "\n",
    "    return anova_df, ttest_df, permanova_disease_value, permanova_status_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bbb956db-d1f3-478d-898e-83e6cea1e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_model(params, train_taxonomy, train_pathways):\n",
    "    \n",
    "    batch_size    = params['batch_size']\n",
    "    lr            = params['lr']\n",
    "    embedding_dim = params['embedding_dim']\n",
    "    latent_dim    = params['latent_dim']\n",
    "    \n",
    "    X_train_taxonomy, X_test_taxonomy = train_test_split(train_taxonomy*100, test_size=0.2, random_state=0)\n",
    "    X_train_pathways, X_test_pathways = train_test_split(train_pathways*100, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Model and Data Parallelization\n",
    "    num_bacteria = train_taxonomy.shape[1]\n",
    "    num_pathways = train_pathways.shape[1]\n",
    "\n",
    "    model = BacteriaModel(num_pathways, num_bacteria, embedding_dim, latent_dim).to(DEVICE)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Data Preparation\n",
    "    X_train_taxonomy_tensor     = torch.tensor(X_train_taxonomy.values).float().unsqueeze(-1)  \n",
    "    X_train_taxonomy_dataloader = DataLoader(X_train_taxonomy_tensor,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    X_train_pathways_tensor     = torch.tensor(X_train_pathways.values).float().unsqueeze(-1)  \n",
    "    X_train_pathways_dataloader = DataLoader(X_train_pathways_tensor,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    X_test_taxonomy_tensor      = torch.tensor(X_test_taxonomy.values).float().unsqueeze(-1)  \n",
    "    X_test_taxonomy_dataloader  = DataLoader(X_test_taxonomy_tensor,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)  \n",
    "    \n",
    "    X_test_pathways_tensor      = torch.tensor(X_test_pathways.values).float().unsqueeze(-1)  \n",
    "    X_test_pathways_dataloader  = DataLoader(X_test_pathways_tensor,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)  \n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training Loop\n",
    "    history = []\n",
    "\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "            \n",
    "        training_loss      = 0.0\n",
    "        train_steps        = 0.0\n",
    "        train_taxonomy_mse = 0.0\n",
    "        train_pathways_mse = 0.0\n",
    "        \n",
    "        for xT, xP in zip(X_train_taxonomy_dataloader, X_train_pathways_dataloader):\n",
    "            \n",
    "            xT = xT.to(DEVICE)\n",
    "            xP = xP.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()     \n",
    "    \n",
    "            xT_pred, xP_pred, z = model(xP, xT)\n",
    "            loss = loss_function(xT, xT_pred, xP, xP_pred)  \n",
    "            \n",
    "            taxonomy_mse = nn.functional.mse_loss(xT_pred, xT, reduction='mean')\n",
    "            pathways_mse = nn.functional.mse_loss(xP_pred, xP, reduction='mean')\n",
    "    \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss      += loss.item()\n",
    "            train_taxonomy_mse += taxonomy_mse.item()\n",
    "            train_pathways_mse += pathways_mse.item()\n",
    "            train_steps        += 1\n",
    "    \n",
    "        model.eval()\n",
    "        \n",
    "        test_loss               = 0.0 \n",
    "        test_steps              = 0.0\n",
    "        test_taxonomy_mse       = 0.0\n",
    "        test_pathways_mse       = 0.0\n",
    "        test_taxonomy_disparity = 0.0\n",
    "        test_pathways_disparity = 0.0 \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for xT, xP in zip(X_test_taxonomy_dataloader, X_test_pathways_dataloader):\n",
    "                \n",
    "                xT = xT.to(DEVICE)\n",
    "                xP = xP.to(DEVICE)\n",
    "            \n",
    "                xT_pred, xP_pred, z = model(xP, xT)\n",
    "                loss = loss_function(xT, xT_pred, xP, xP_pred)  \n",
    "    \n",
    "                test_taxonomy_disparity += calculate_disparity(xT_pred, xT)\n",
    "                test_pathways_disparity += calculate_disparity(xP_pred, xP)\n",
    "       \n",
    "                taxonomy_mse = nn.functional.mse_loss(xT_pred, xT, reduction='mean')\n",
    "                pathways_mse = nn.functional.mse_loss(xP_pred, xP, reduction='mean')\n",
    "                \n",
    "                test_loss         += loss.item()\n",
    "                test_taxonomy_mse += taxonomy_mse.item()\n",
    "                test_pathways_mse += pathways_mse.item()\n",
    "                test_steps        += 1\n",
    "                \n",
    "        history.append({\"epoch\":epoch,\n",
    "                     \"train_loss\":training_loss/train_steps,\n",
    "                     \"train_taxonomy_mse\":train_taxonomy_mse/train_steps,\n",
    "                     \"train_pathways_mse\":train_pathways_mse/train_steps,\n",
    "                     \"test_loss\":test_loss/test_steps,\n",
    "                     \"test_taxonomy_mse\":test_taxonomy_mse/test_steps,\n",
    "                     \"test_pathways_mse\":test_pathways_mse/test_steps,\n",
    "                     \"test_taxonomy_disparity\":test_taxonomy_disparity/test_steps,\n",
    "                     \"test_pathways_disparity\":test_pathways_disparity/test_steps})\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db6dccb-3d59-4737-a8cb-29a288ca40df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-14 14:01:44,324] A new study created in memory with name: no-name-196fd655-d319-45bd-b8f5-2c69b00e082a\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Separate data into validation and train set\n",
    "val_idx_sick = (\n",
    "        metadata[metadata['study_condition'].isin(['IBD', 'hypertension', 'CRC', 'T2D', 'T1D', 'cirrhosis'])]\n",
    "        .groupby('study_condition')\n",
    "        .sample(n=200 // 6, random_state=42).sample_id\n",
    ")\n",
    "\n",
    "val_idx_healthy = (\n",
    "        metadata[metadata['study_condition'] == 'control']\n",
    "        .sample(n=100, random_state=42).sample_id\n",
    ")\n",
    "\n",
    "train_idx = metadata[~metadata.sample_id.isin(val_idx_sick + val_idx_healthy)].sample_id\n",
    "val_idx   = val_idx_sick.tolist() + val_idx_healthy.tolist() \n",
    "\n",
    "filtered_pathways_train = filtered_pathways[filtered_pathways.index.isin(train_idx)]\n",
    "filtered_taxonomy_train = filtered_taxonomy[filtered_taxonomy.index.isin(train_idx)]\n",
    "\n",
    "filtered_pathways_val = filtered_pathways[filtered_pathways.index.isin(val_idx)] *100\n",
    "filtered_taxonomy_val = filtered_taxonomy[filtered_taxonomy.index.isin(val_idx)] *100\n",
    "\n",
    "\n",
    "# Storage for results\n",
    "trial_results     = []\n",
    "trained_models    = {}\n",
    "combined_anova_df = []\n",
    "combined_ttest_df = []\n",
    "permanova_df      = []\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna hyperparameter tuning.\"\"\"\n",
    "    \n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    lr            = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2) \n",
    "    embedding_dim =  trial.suggest_categorical(\"embedding_dim\", [32, 64, 128])\n",
    "    latent_dim    =  trial.suggest_categorical(\"latent_dim\", [4, 8, 16, 32, 64])\n",
    "\n",
    "\n",
    "\n",
    "    params = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"latent_dim\": latent_dim\n",
    "    }\n",
    "\n",
    "    history, model = run_model(params, filtered_taxonomy_train, filtered_pathways_train)\n",
    "\n",
    "    best_test_loss = min([epoch[\"test_loss\"] for epoch in history])\n",
    "\n",
    "    anova_df, ttest_df, permanova_disease_value, permanova_status_value = run_latent_analysis(model, filtered_taxonomy_val, filtered_pathways_val)\n",
    "    \n",
    "    trial_results.append({\n",
    "        \"trial\": trial.number,\n",
    "        \"params\": params,\n",
    "        \"history\": history,\n",
    "        \"best_test_loss\": best_test_loss\n",
    "    })\n",
    "\n",
    "    combined_anova_df.append(anova_df)\n",
    "    combined_ttest_df.append(ttest_df)\n",
    "\n",
    "    permanova_df.append({\"trial\":trial.number, \n",
    "                         \"disease_pvalue\": permanova_disease_value,\n",
    "                         \"status_pvalue\":permanova_status_value})\n",
    "    # Save the trained model\n",
    "    trained_models[trial.number] = model.state_dict()\n",
    "\n",
    "    return best_test_loss  # We aim to minimize test loss\n",
    "\n",
    "# Run Optuna optimization\n",
    "\n",
    "n_trials = 20\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize validation loss\n",
    "study.optimize(objective, n_trials=n_trials)  # Run 10 trials\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(trial_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481ff9f-8e3d-49f2-a0ae-16192f52045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_DF = pd.DataFrame()\n",
    "for i in range(num_trials):\n",
    "    \n",
    "    df = pd.DataFrame(trial_results[i]['history'])\n",
    "    df['trial'] = trial_results[i]['trial']\n",
    "    df[['batch_size', 'lr', 'embedding_dim', 'latent_dim']] = list(trial_results[i]['params'].values())\n",
    "\n",
    "    TRIAL_DF = pd.concat([TRIAL_DF, df])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
